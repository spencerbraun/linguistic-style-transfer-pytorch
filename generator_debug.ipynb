{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from linguistic_style_transfer_pytorch.config import GeneralConfig, ModelConfig\n",
    "from linguistic_style_transfer_pytorch.data_loader import TextDataset\n",
    "from linguistic_style_transfer_pytorch.model import AdversarialVAE\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gconfig = GeneralConfig()\n",
    "config = GeneralConfig()\n",
    "mconfig = ModelConfig()\n",
    "\n",
    "# load word embeddings\n",
    "weights = torch.FloatTensor(np.load(gconfig.word_embedding_path))\n",
    "# load checkpoint\n",
    "model_checkpoint = torch.load('linguistic_style_transfer_pytorch/checkpoints/model_epoch_20.pt')\n",
    "# Load model\n",
    "model = AdversarialVAE(weight=weights)\n",
    "model.load_state_dict(model_checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# Load average style embeddings\n",
    "with open(config.avg_style_emb_path, 'rb') as f:\n",
    "    avg_style_embeddings = pickle.load(f)\n",
    "# set avg_style_emb attribute of the model\n",
    "model.avg_style_emb = avg_style_embeddings\n",
    "# load word2index\n",
    "with open(gconfig.w2i_file_path) as f:\n",
    "    word2index = json.load(f)\n",
    "# load index2word\n",
    "with open(gconfig.i2w_file_path) as f:\n",
    "    index2word = json.load(f)\n",
    "label2index = {'neg': 0, 'pos': 1}\n",
    "# Read input sentence\n",
    "source_sentence = \"this soup is good\"#input(\"Enter the source sentence\")\n",
    "target_style = \"neg\"#input(\"Enter the target style: pos or neg\")\n",
    "# Get token ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "token_ids = [word2index.get(word, gconfig.unk_token)\n",
    "             for word in source_sentence.split()]\n",
    "token_ids = torch.LongTensor(token_ids)\n",
    "target_style_id = torch.LongTensor(label2index[target_style])\n",
    "# Get transfered sentence token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([[1,2,3,4,5,6,7,8]]).view([8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.repeat(128).view([128,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.array([4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor([0, 1]) == torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(mode='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=mconfig.batch_size)\n",
    "for iteration, batch in enumerate(tqdm(test_dataloader)):\n",
    "    \n",
    "    # unpacking\n",
    "    sequences, seq_lens, labels, bow_rep = batch\n",
    "    print(sequences.shape, seq_lens.shape)\n",
    "    model.transfer_style(sequences, seq_lens.view([128]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(10, 20, 2, batch_first=True)\n",
    "input_ = torch.autograd.Variable(torch.randn(5, 3, 10).transpose(0, 1))\n",
    "h0 = torch.autograd.Variable(torch.randn(2, 3, 20))\n",
    "# output, hn = rnn(input, h0)\n",
    "input_.size(), h0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconfig.embedding_size + mconfig.generative_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconfig.hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "target_tokenids = model.transfer_style(token_ids, target_style_id)\n",
    "target_sentence = \"\".join([index2word.get(idx) for idx in target_tokenids])\n",
    "print(\"Style transfered sentence: {}\".format(target_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from linguistic_style_transfer_pytorch.config import GeneralConfig, ModelConfig\n",
    "from linguistic_style_transfer_pytorch.data_loader import TextDataset\n",
    "from linguistic_style_transfer_pytorch.model import AdversarialVAE\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "gconfig = GeneralConfig()\n",
    "config = GeneralConfig()\n",
    "mconfig = ModelConfig()\n",
    "\n",
    "# load word embeddings\n",
    "weights = torch.FloatTensor(np.load(gconfig.word_embedding_path))\n",
    "# load checkpoint\n",
    "model_checkpoint = torch.load('linguistic_style_transfer_pytorch/checkpoints/model_epoch_20.pt')\n",
    "# Load model\n",
    "model = AdversarialVAE(weight=weights)\n",
    "model.load_state_dict(model_checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# Load average style embeddings\n",
    "with open(config.avg_style_emb_path, 'rb') as f:\n",
    "    avg_style_embeddings = pickle.load(f)\n",
    "# set avg_style_emb attribute of the model\n",
    "model.avg_style_emb = avg_style_embeddings\n",
    "# load word2index\n",
    "with open(gconfig.w2i_file_path) as f:\n",
    "    word2index = json.load(f)\n",
    "# load index2word\n",
    "with open(gconfig.i2w_file_path) as f:\n",
    "    index2word = json.load(f)\n",
    "label2index = {'neg': 0, 'pos': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if use_cuda:\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "#=============== Define dataloader ================#\n",
    "test_dataset = TextDataset(mode='train')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=mconfig.batch_size)\n",
    "content_discriminator_params, style_discriminator_params, vae_and_classifier_params = model.get_params()\n",
    "#model.transfer_style(sequences, seq_lens.view([128]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, seq_lens, labels, bow_rep = [x for x in test_dataloader][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    seq_lengths = seq_lens.view([128]).cuda()\n",
    "    sequences = sequences.cuda()\n",
    "else:\n",
    "    seq_lengths = seq_lens.view([128])\n",
    "seq_lens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.transfer_style(self, sequences, seq_lengths, style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 40])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths, perm_index = seq_lengths.sort(descending=True)\n",
    "sequences = sequences[perm_index]\n",
    "print(sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([39, 38, 36, 35, 35, 34, 34, 34, 34, 34, 34, 34, 33, 33, 33, 32, 32, 31,\n",
      "        30, 30, 30, 29, 28, 28, 28, 27, 27, 27, 26, 26, 26, 25, 25, 25, 24, 24,\n",
      "        24, 23, 23, 23, 23, 23, 23, 23, 22, 21, 21, 21, 20, 20, 19, 19, 19, 18,\n",
      "        18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 13, 13, 13, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10,\n",
      "        10, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,  9,  8,  8,  8,  7,\n",
      "         7,  6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 40, 300])\n"
     ]
    }
   ],
   "source": [
    "embedded_seqs = model.embedding(sequences)\n",
    "print(embedded_seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 39, 512])\n"
     ]
    }
   ],
   "source": [
    "#seq_lengths = torch.from_numpy(np.array([15]))\n",
    "packed_seqs = pack_padded_sequence(\n",
    "    embedded_seqs, lengths=seq_lengths, batch_first=True)\n",
    "packed_output, (_) = model.encoder(packed_seqs)\n",
    "output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 512])\n",
      "torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "sentence_emb = output[torch.arange(output.size(0)), seq_lengths-1]\n",
    "print(sentence_emb.shape)\n",
    "\n",
    "content_emb_mu, content_emb_log_var = model.get_content_emb(\n",
    "    sentence_emb)\n",
    "print(content_emb_mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "sampled_content_emb = model.sample_prior(\n",
    "    content_emb_mu, content_emb_log_var)\n",
    "print(sampled_content_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8])\n"
     ]
    }
   ],
   "source": [
    "target_style_emb = model.avg_style_emb[1].repeat(128).view([128,8])\n",
    "print(target_style_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_style_emb.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_emb = torch.cat(\n",
    "    (target_style_emb, sampled_content_emb), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 136])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9483, -0.9371, -1.1488,  ...,  2.2135,  1.3757, -0.6736],\n",
       "        [ 0.9483, -0.9371, -1.1488,  ...,  2.2606,  1.1101, -0.4844],\n",
       "        [ 0.9483, -0.9371, -1.1488,  ...,  2.2933,  1.2413, -0.6126],\n",
       "        ...,\n",
       "        [ 0.9483, -0.9371, -1.1488,  ...,  2.2160,  1.3934, -0.7832],\n",
       "        [ 0.9483, -0.9371, -1.1488,  ...,  3.0279,  1.3415, -0.3892],\n",
       "        [ 0.9483, -0.9371, -1.1488,  ...,  1.7875,  1.4792, -0.7660]],\n",
       "       device='cuda:0', grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self.generate_sentences(self, input_sentences, latent_emb, inference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfered_sentence = self.generate_sentences(\n",
    "#     input_sentences=None, latent_emb=generative_emb, inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = sequences\n",
    "latent_emb = generative_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 136])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sos_token_tensor = torch.tensor(\n",
    "#         [gconfig.predefined_word_index['<sos>']], \n",
    "#         device=input_sentences.device).unsqueeze(0).repeat(mconfig.batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sos_token_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sentences = torch.cat(\n",
    "#         (sos_token_tensor, input_sentences), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embs = model.embedding(input_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40, 300])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_emb = latent_emb.unsqueeze(1).repeat(\n",
    "#         1, mconfig.max_seq_len+1, 1)\n",
    "latent_emb = latent_emb.unsqueeze(1).repeat(\n",
    "        1, mconfig.max_seq_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 136])\n",
      "torch.Size([128, 40, 136])\n"
     ]
    }
   ],
   "source": [
    "print(generative_emb.shape)\n",
    "print(latent_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sent_embs = torch.cat(\n",
    "        (sentence_embs.cuda(), latent_emb), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40, 436])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sent_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_sentences = torch.zeros(\n",
    "#     mconfig.max_seq_len, mconfig.batch_size, \n",
    "#     device=input_sentences.device)\n",
    "output_sentences = torch.zeros(\n",
    "        mconfig.max_seq_len, mconfig.batch_size, mconfig.vocab_size, device=input_sentences.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 128, 9203])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "hidden_states = torch.zeros(\n",
    "        mconfig.batch_size, mconfig.hidden_dim, device=input_sentences.device)\n",
    "print(mconfig.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mconfig.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.cuda().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(mconfig.max_seq_len):\n",
    "    # get words at the index idx from all the batches\n",
    "    words = gen_sent_embs[:, idx, :]\n",
    "    hidden_states = model.decoder(words, hidden_states)\n",
    "    # project over vocab space\n",
    "    next_word_logits = model.projector(hidden_states)\n",
    "#     next_word = nn.Softmax(dim=1)(next_word_logits).argmax(dim=1)\n",
    "    output_sentences[idx] = next_word_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9203])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences[0,0,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 40])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 9203])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = output_sentences.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "transact the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "<unk> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "transact the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "ai the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "<unk> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "holes the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "<unk> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "ai the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "transact the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "<unk> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "<unk> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tiffey the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "for i in range(mconfig.batch_size):\n",
    "    sent = argmax[:,i]\n",
    "    print(' '.join([index2word[str(int(word))] for word in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5120, 9203])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences.view(-1, mconfig.vocab_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9203])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_logits[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9203])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_word_probs = nn.Softmax(dim=1)(self.projector(hidden_states))\n",
    "next_word = max(next_word_probs.argmax(1))\n",
    "output_sentence[idx] = next_word\n",
    "word_emb = self.embedding(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoder(output_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9203])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 128, 9203])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training mode\n",
    "if not inference:\n",
    "    # Prepend the input sentences with <sos> token\n",
    "    sos_token_tensor = torch.tensor(\n",
    "        [gconfig.predefined_word_index['<sos>']], \n",
    "        device=input_sentences.device).unsqueeze(0).repeat(mconfig.batch_size, 1)\n",
    "    input_sentences = torch.cat(\n",
    "        (sos_token_tensor, input_sentences), dim=1)\n",
    "    sentence_embs = self.dropout(self.embedding(input_sentences))\n",
    "    # Make the latent embedding compatible for concatenation\n",
    "    # by repeating it for max_seq_len + 1(additional one bcoz <sos> tokens were added)\n",
    "    latent_emb = latent_emb.unsqueeze(1).repeat(\n",
    "        1, mconfig.max_seq_len+1, 1)\n",
    "    gen_sent_embs = torch.cat(\n",
    "        (sentence_embs, latent_emb), dim=2)\n",
    "    # Delete latent embedding and sos token tensor to reduce memory usage\n",
    "    del latent_emb, sos_token_tensor\n",
    "    output_sentences = torch.zeros(\n",
    "        mconfig.max_seq_len, mconfig.batch_size, mconfig.vocab_size, device=input_sentences.device)\n",
    "    # initialize hidden state\n",
    "    hidden_states = torch.zeros(\n",
    "        mconfig.batch_size, mconfig.hidden_dim, device=input_sentences.device)\n",
    "    # generate sentences one word at a time in a loop\n",
    "    for idx in range(mconfig.max_seq_len):\n",
    "        # get words at the index idx from all the batches\n",
    "        words = gen_sent_embs[:, idx, :]\n",
    "        hidden_states = self.decoder(words, hidden_states)\n",
    "        # project over vocab space\n",
    "        next_word_logits = self.projector(hidden_states)\n",
    "        output_sentences[idx] = next_word_logits\n",
    "# if inference mode is on\n",
    "else:\n",
    "\n",
    "    sos_token_tensor = torch.tensor(\n",
    "        [gconfig.predefined_word_index['<sos>']], device=latent_emb.device).unsqueeze(0).repeat(mconfig.batch_size, 1)\n",
    "    word_emb = self.embedding(sos_token_tensor)\n",
    "    hidden_states = torch.zeros(\n",
    "        mconfig.batch_size, mconfig.hidden_dim, device=latent_emb.device)\n",
    "    # Store output sentences\n",
    "    output_sentences = torch.zeros(\n",
    "        mconfig.max_seq_len, 1, device=latent_emb.device)\n",
    "    latent_emb = latent_emb.unsqueeze(1)#.repeat(\n",
    "        #1, mconfig.max_seq_len+1, 1)\n",
    "    with torch.no_grad():\n",
    "        # Greedily generate new words at a time\n",
    "        for idx in range(mconfig.max_seq_len):\n",
    "            gen_sent_embs = torch.cat(\n",
    "                (word_emb, latent_emb), dim=2)\n",
    "            words = gen_sent_embs[:, idx, :]\n",
    "            hidden_states = self.decoder(words, hidden_states)\n",
    "            next_word_probs = nn.Softmax(dim=1)(\n",
    "                self.projector(hidden_states))\n",
    "            next_word = max(next_word_probs.argmax(1))\n",
    "            output_sentence[idx] = next_word\n",
    "            word_emb = self.embedding(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
